[{"categories":["网络"],"content":"介绍 Kubernetes 提供了多种对外暴露服务的方式 对于开发调试来说，使用 VPN 方式打通 本机开发环境 与 Cluster 之间的网络，会变得十分方便 ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:1:0","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"SoftEther VPN 选用 SoftEther VPN 主要原因是 支持通过单端口多种VPN接入方式，比如 SoftEtherVPN Client、OpenVPN 有统一的账户管理、分组管理、权限控制、拆分隧道 等功能 ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:1:1","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"部署介绍 自定义部署：支持拆分隧道功能，需要下载客户端进行管理 无配置快速部署：不支持拆分隧道功能，可以不连接远程管理，开箱即用 ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:1:2","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"自定义部署 使用镜像 abyssviper/softethervpn ，该镜像编译基于 alpine 基础镜像编译，仅保留了 vpnserver, 非常轻量 并且去除了 拆分隧道 限制（参考该文章），可以向 VPN 客户端 推送定制路由 ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:2:0","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"完整配置文件 创建发布配置文件 deployment-softethervpn.yaml, 配置文件的 Deployment 命名存在一定问题，请看本章疑问点部分 apiVersion:apps/v1kind:Deploymentmetadata:name:vpnnamespace:devopsspec:selector:matchLabels:app:softether-vpnservertemplate:metadata:labels:app:softether-vpnserverspec:containers:- name:softether-vpn-alpineimage:abyssviper/softethervpnimagePullPolicy:IfNotPresentports:- containerPort:5555name:connectprotocol:TCPlivenessProbe:tcpSocket:port:5555initialDelaySeconds:60timeoutSeconds:5failureThreshold:12readinessProbe:tcpSocket:port:5555resources:limits:cpu:1000mmemory:200Mirequests:cpu:500mmemory:100MivolumeMounts:- name:softether-vpn-storgesubPath:softethervpn/vpn_server.configmountPath:/opt/vpnserver/vpn_server.config- name:softether-vpn-storgesubPath:softethervpn/server_logmountPath:/opt/vpnserver/server_log- name:softether-vpn-storgesubPath:softethervpn/packet_logmountPath:/opt/vpnserver/packet_log- name:softether-vpn-storgesubPath:softethervpn/security_logmountPath:/opt/vpnserver/security_logvolumes:- name:softether-vpn-storgepersistentVolumeClaim:claimName:vpn-pvc---apiVersion:v1kind:Servicemetadata:name:vpnnamespace:devopsspec:selector:app:softether-vpnservertype:NodePortports:- name:connectport:5555nodePort:30003 创建PV, PVC配置文件 pvc-softethervpn.yaml 本文使用 NFS apiVersion:v1kind:PersistentVolumemetadata:name:vpn-pvspec:capacity:storage:100GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:Deletenfs:server:192.168.7.40path:/data/kubernetes---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:vpn-pvcnamespace:devopsspec:accessModes:- ReadWriteManyresources:requests:storage:1Gi 首先创建发布 PV, PVC 配置 $ kubectl apply -f pvc-softethervpn.yaml 然后发布SoftEtherVPN $ kubectl apply -f deployment-softethervpn.yaml ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:2:1","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"管理 管理工具下载 通过 SoftEtherVPN 官方提供的 Manager 工具进行管理（下载地址） 管理工具下载地址管理工具下载地址 \"\r管理工具下载地址\r 连接配置 配置服务端地址以及端口， 第一次连接需要手动设置管理密码 服务端连接配置服务端连接配置 \"\r服务端连接配置\r 配置 SecureNAT 和 用户 连接成功后，通过 管理虚拟HUB(A) —》虚拟 NAT 和 虚拟 DHCP 服务器(V) —》启用 SecureNAT(E) —》SecureNAT配置(C) SecureNAT配置SecureNAT配置 \"\rSecureNAT配置\r 网关配置：如果填写，则本地VPN客户端流量默认路由走服务端，不需要推送路由；推荐使用本地网关+路由推送的方式 DNS配置：如果需要使用DNS解析（example: my-nginx.default.svc.cluster.local），需要配置集群内 CoreDNS 的 IP 地址 路由推送：路由推送格式如图所示，IP网络地址/子网掩码/网关IP地址，网关IP地址填写 SecureNAT 配置的网关地址; 一般需要推送 Kubernetes Calico 网段 以及 SVC网段 在 管理虚拟HUB(A) 中，添加一个用户 添加用户添加用户 \"\r添加用户\r 配置OpenVPN 通过 VPN 管理工具，启用 OpenVPN 功能，生成 OpenVPN Client 配置样本文件 OpenVPN配置OpenVPN配置 \"\rOpenVPN配置\r 解压后，对 access_l3.ovpn 进行编辑 OpnVPN配置案例文件OpnVPN配置案例文件 \"\rOpnVPN配置案例文件\r 按照如下的配置进行修改，其中 remote: 上述 Kubernetes 发布 VPN 配置文件的 Service 地址 proto: 使用 TCP 作为连接协议，也可以使用 UDP，注意修改发布 VPN 时 YAML 配置文件的 端口映射配置 ca: 添加 ca标签，填写 VPN Server 配置中的 证书信息，证书信息获取方式见后面 dev tun proto tcp remote 192.168.7.200 30003 cipher AES-128-CBC auth SHA1 resolv-retry infinite nobind persist-key persist-tun client verb 3 auth-user-pass \u003cca\u003e -----BEGIN CERTIFICATE----- 从如下配置中获取 -----END CERTIFICATE----- \u003c/ca\u003e 可以通过打开 编辑设置(D) 找到证书信息 或 加密与网络(E) 导出配置文件 获取ca信息方式1获取ca信息方式1 \"\r获取ca信息方式1\r 获取ca信息方式2获取ca信息方式2 \"\r获取ca信息方式2\r ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:2:2","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"连接 SoftEtherVPN Client 通过官网下载连接客户端，适用于 Windows / Linux；推荐 Linux 与 OSX 中使用 OpenVPN 的方式 需要填写 主机名、端口号、虚拟HUB名、账号密码 SoftEtherVPN Client 支持同时连接多个VPNServer，只需要创建多个虚拟网络适配器即可 SoftEtherVPN Client 连接配置SoftEtherVPN Client 连接配置 \"\rSoftEtherVPN Client 连接配置\r OpenVPN 下载Open VPN 客户端： Windows下载地址， OSX下载地址 通过上述配置的配置文件即可进行连接 SoftEtherVPN Server 可以设置多个虚拟 HUB；对于OpenVPN来说，通过 用户名@HUB 的方式指定 HUB，直接使用用户名默认是 Default HUB 例如：VPN HUB中的 test 用户，用户名为：test@VPN OpenVPN 客户端连接OpenVPN 客户端连接 \"\rOpenVPN 客户端连接\r ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:2:3","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"验证 VPN连接成功后，可以进行 DNS 解析设置 以及 访问测试 DNS解析设置DNS解析设置 \"\rDNS解析设置\r 以 ArgoCD 为例进行访问测试 访问测试访问测试 \"\r访问测试\r ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:2:4","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"数据持久化 配置的持久化: 只需要持久化 vpn_server.config 即可 日志的持久化: server_log packet_log security_log 三个文件夹持久化即可 注意点: 需要注意的是，SoftEtherVPN Server 并不会在更改配置（添加用户，更改SecuretNAT等）后马上写入 vpn_server.config, 需要等待一段时间间隔才会落地到 vpn_server.config 文件 ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:2:5","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"无配置快速部署 快速部署配置，选型的镜像为 siomiz/softethervpn, 该镜像同样有基于 alpine 版本，并且会启动容器时进行快速初始化，并创建可用的账户，非常方便；不过该镜像 不支持拆分隧道功能 ，无法推送定制路由 ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:3:0","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"配置文件 创建发布配置文件 deployment-softethervpn.yaml apiVersion:apps/v1kind:Deploymentmetadata:name:vpnnamespace:devopsspec:selector:matchLabels:app:softether-vpnservertemplate:metadata:labels:app:softether-vpnserverspec:containers:- name:softether-vpn-alpineimage:siomiz/softethervpnimagePullPolicy:IfNotPresentports:- containerPort:5555name:connectprotocol:TCPlivenessProbe:tcpSocket:port:5555initialDelaySeconds:60timeoutSeconds:5failureThreshold:12readinessProbe:tcpSocket:port:5555resources:limits:cpu:1000mmemory:200Mirequests:cpu:500mmemory:100Mi---apiVersion:v1kind:Servicemetadata:name:vpnnamespace:devopsspec:selector:app:softether-vpnservertype:NodePortports:- name:connectport:5555nodePort:30003 发布到 Kubernetes 中 $ kubectl apply -f deployment-softethervpn.yaml ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:3:1","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"查看连接信息 通过 logs 查看日志信息，从中查看默认生成的 SoftEtherVPN 账户 以及 OpenVPN 配置文件 $ kubectl logs -f -n devops vpn-b55fb8f4-jmxqh 如下，可以看出 用户名为 user9703 密码为 6758.1071.6532.2086.9735, OpenVPN 配置文件 等信息 OpenVPN 连接的 proto 以及 remote 信息需要更改为合适的，具体参照自定义部署 # [!!] This image requires --cap-add NET_ADMIN # ======================== # user9703 # 6758.1071.6532.2086.9735 # ======================== # Version 4.34 Build 9745 (English) dev tun proto udp remote _unregistered_vpn528125132.v4.softether.net 1194 ;http-proxy-retry ;http-proxy [proxy server] [proxy port] cipher AES-128-CBC auth SHA1 resolv-retry infinite nobind persist-key persist-tun client verb 3 auth-user-pass \u003cca\u003e -----BEGIN CERTIFICATE----- MIIDyjCCArKgAwIBAgIBADANBgkqhkiG9w0BAQsFADBkMRswGQYDVQQDDBJ2cG4t YjU1ZmI4ZjQtam14cWgxGzAZBgNVBAoMEnZwbi1iNTVmYjhmNC1qbXhxaDEbMBkG A1UECwwSdnBuLWI1NWZiOGY0LWpteHFoMQswCQYDVQQGEwJVUzAeFw0yMDA4MzEx NjM2NDNaFw0zNzEyMzExNjM2NDNaMGQxGzAZBgNVBAMMEnZwbi1iNTVmYjhmNC1q bXhxaDEbMBkGA1UECgwSdnBuLWI1NWZiOGY0LWpteHFoMRswGQYDVQQLDBJ2cG4t YjU1ZmI4ZjQtam14cWgxCzAJBgNVBAYTAlVTMIIBIjANBgkqhkiG9w0BAQEFAAOC AQ8AMIIBCgKCAQEA9j++0cYr7/1enukSjhzA37s01SWNazUcpgEjrclfikuzKiw0 M7bJGEjM8eJTUqvtIwJOkWVbrVfVTX1zV/yCenFns05WRSud2oEGyXWh0oa8aChv w/S+KYdGub4sLkwDbIfGEhJQIXO3iQ9ecdjX+QFUlOL7PdCDyxc6wao2ZsjwCeLt oamj8AOVH+w0E24OC0H3eiJ5YMKWo56JwH0spbwl/xONq1PfUuP494dG6C7sOMWS DIW3OD3Bo071B9A5OGtE/fRUe56ZxsOZySlhaI1Yl8LZvZtSdkAhLByKYTjmKd7J NbCWJUMiLCSIxRFAjxCDjmBrEBGkAtM4v+PC1wIDAQABo4GGMIGDMA8GA1UdEwEB /wQFMAMBAf8wCwYDVR0PBAQDAgH2MGMGA1UdJQRcMFoGCCsGAQUFBwMBBggrBgEF BQcDAgYIKwYBBQUHAwMGCCsGAQUFBwMEBggrBgEFBQcDBQYIKwYBBQUHAwYGCCsG AQUFBwMHBggrBgEFBQcDCAYIKwYBBQUHAwkwDQYJKoZIhvcNAQELBQADggEBALRC 1HKokh3KwpgKjznMwOR83bPu8QveHWr0GrlzseKxqHGJcTy0sxnkfk3mAu9v8m4a UACj3H0opouRAqOTdbogCWXcERwLM1084wehyeUZKX9gfcWGbAPWVjcY1kC5KePs IXWhEMC56wIGMFs4mS5vx7aNVE9k4Ssrnf7T3mkM/ACrN9dg+/H2CVxNr5FTQIwy IGTC3AP5WLPVfEk5SByEOZqFRiBIDDhvKU4gT4cD2+FHLM6OM8Z09qGs8uq6KLr6 LfUjc/c5CI+FInmm1hLB3NZug17TEaVchXeQNs921wKOOWoCKucToOPXkwYE1V/c zc7doXSaSkrKyIwCqCY= -----END CERTIFICATE----- \u003c/ca\u003e ;\u003ccert\u003e ;-----BEGIN CERTIFICATE----- ; ;-----END CERTIFICATE----- ;\u003c/cert\u003e ;\u003ckey\u003e ;-----BEGIN RSA PRIVATE KEY----- ; ;-----END RSA PRIVATE KEY----- ;\u003c/key\u003e # Creating user(s): user9703 # [initial setup OK] ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:3:2","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"验证 SoftEtherVPN Client 连接后，访问验证 SoftEtherVPN Client验证SoftEtherVPN Client验证 \"\rSoftEtherVPN Client验证\r ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:3:3","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"其他配置 该镜像提供了很多初始化配置信息，具体可以参考官方配置信息 ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:3:4","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["网络"],"content":"疑问点 上述的 Deployment 配置文件存在如下问题： 部分 deployment name 会导致 开启 SecureNAT 以后， 客户端连接后，立即断开 客户端与服务端的 连接 name 为 vpn centos7 vpn-s 之类测试的可以正常使用，vpnserver vpn-server softether-vpn 之类的就会断开 该问题暂时没有找到原因，如果有了解的，期望您留言指教，感谢 ","date":"2020-08-27","objectID":"/softether-vpn-k8s/:4:0","tags":["VPN","网络"],"title":"Softether VPN 打通容器调试网络","uri":"/softether-vpn-k8s/"},{"categories":["Kubernetes"],"content":"前言 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:1:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"系列目录 二进制部署Kubernetes Part1 二进制部署Kubernetes Part2 二进制部署Kubernetes Part3 二进制部署Kubernetes Part4(本篇) ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:1:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"注意点 本章节所有操作默认均在 k8s-node01下操作 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:1:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"CoreDNS ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:2:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"下载 cd /opt/k8s/work git clone https://github.com/coredns/deployment.git mv deployment coredns-deployment ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:2:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"创建CoreDNS cd /opt/k8s/work/coredns-deployment/kubernetes source /opt/k8s/bin/environment.sh ./deploy.sh -i ${CLUSTER_DNS_SVC_IP} -d ${CLUSTER_DNS_DOMAIN} | kubectl apply -f - ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:2:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"检查功能 确保 Ready 与 Status正常 kubectl get all -n kube-system -l k8s-app=kube-dns 新建一个Deployment 并发布 cd /opt/k8s/work cat \u003e my-nginx.yaml \u003c\u003cEOF apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: replicas: 2 selector: matchLabels: run: my-nginx template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx:1.7.9 ports: - containerPort: 80 EOF kubectl create -f my-nginx.yaml export 该 Deployment 生成的 ，生成 my-nginx 服务 kubectl expose deploy my-nginx $ kubectl get services my-nginx -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR my-nginx ClusterIP 10.254.251.214 \u003cnone\u003e 80/TCP 10s run=my-nginx 创建另一个 Pod，查看 /etc/resolv.conf 是否包含 kubelet 配置的 --cluster-dns 和 --cluster-domain，是否能够将服务 my-nginx 解析到上面显示的 Cluster IP 10.254.251.214 cd /opt/k8s/work cat \u003e dnsutils-ds.yml \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: dnsutils-ds labels: app: dnsutils-ds spec: type: NodePort selector: app: dnsutils-ds ports: - name: http port: 80 targetPort: 80 --- apiVersion: apps/v1 kind: DaemonSet metadata: name: dnsutils-ds labels: addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: app: dnsutils-ds template: metadata: labels: app: dnsutils-ds spec: containers: - name: my-dnsutils image: tutum/dnsutils:latest command: - sleep - \"3600\" ports: - containerPort: 80 EOF kubectl create -f dnsutils-ds.yml $ kubectl get pods -lapp=dnsutils-ds -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dnsutils-ds-2dpjw 1/1 Running 0 33m 172.30.135.132 k8s-node03 \u003cnone\u003e \u003cnone\u003e dnsutils-ds-85d6q 1/1 Running 0 33m 172.30.85.195 k8s-node01 \u003cnone\u003e \u003cnone\u003e dnsutils-ds-nwnz4 1/1 Running 0 33m 172.30.58.195 k8s-node02 \u003cnone\u003e \u003cnone\u003e 查看 /etc/resolv.conf $ kubectl -it exec dnsutils-ds-2dpjw cat /etc/resolv.conf search default.svc.cluster.local svc.cluster.local cluster.local nameserver 10.254.0.2 options ndots:5 通过 nslookup 进行解析测试 $ kubectl -it exec dnsutils-ds-2dpjw nslookup kubernetes Server: 10.254.0.2 Address: 10.254.0.2#53 Name: kubernetes.default.svc.cluster.local Address: 10.254.0.1 $ kubectl -it exec dnsutils-ds-2dpjw nslookup www.baidu.com Server: 10.254.0.2 Address: 10.254.0.2#53 Non-authoritative answer: www.baidu.com canonical name = www.a.shifen.com. Name: www.a.shifen.com Address: 180.97.34.94 Name: www.a.shifen.com Address: 180.97.34.96 $ kubectl -it exec dnsutils-ds-2dpjw nslookup my-nginx Server: 10.254.0.2 Address: 10.254.0.2#53 Name: my-nginx.default.svc.cluster.local Address: 10.254.251.214 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:2:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"Dashborad 插件 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:3:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"下载配置 cd /opt/k8s/work wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc4/aio/deploy/recommended.yaml mv recommended.yaml dashboard-recommended.yaml ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:3:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"执行所有定义文件 cd /opt/k8s/work kubectl apply -f dashboard-recommended.yaml ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:3:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"检测运行状态 $ kubectl get pods -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE dashboard-metrics-scraper-7b8b58dc8b-djtvn 1/1 Running 0 16m kubernetes-dashboard-6cfc8c4c9-pw2sx 1/1 Running 0 16m ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:3:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"访问 Dashboard Kubernetes 1.7 以后，dashboard 只允许通过 https访问。如果使用 kube proxy 则必须监听 localhost 或 127.0.0.1。对于 NodePort 没有这个限制，但是仅建议在开发环境中使用。对于不满足这些条件的登录访问，在登录成功后浏览器不跳转，始终停在登录界面 通过 port forward 访问 dashboard kubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 4443:443 --address 0.0.0.0 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:3:4","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"创建登录 Dashboard 的 token 和 kubeconfig 文件 dashboard 默认只 支持 token 认证（不支持 client 证书认证），所以如果使用 Kubeconfig 文件，需要将 token 写入到该文件 创建登录 token 使用输出的 token 登录 kubectl create sa dashboard-admin -n kube-system kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '{print $1}') DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system ${ADMIN_SECRET} | grep -E '^token' | awk '{print $2}') echo ${DASHBOARD_LOGIN_TOKEN} 创建使用 token 的 Kubeconfig 文件 用生成的 dashboard.kubeconfig 登录 Dashboard source /opt/k8s/bin/environment.sh # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/cert/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=dashboard.kubeconfig # 设置客户端认证参数，使用上面创建的 Token kubectl config set-credentials dashboard_user \\ --token=${DASHBOARD_LOGIN_TOKEN} \\ --kubeconfig=dashboard.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=dashboard_user \\ --kubeconfig=dashboard.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=dashboard.kubeconfig ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:3:5","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"部署 kube-prometheus 框架 kube-prometheus 是一整套监控解决方案，它使用 Prometheus 采集集群指标，Grafana 做展示，包含如下组件： The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus Adapter for Kubernetes Metrics APIs （k8s-prometheus-adapter） kube-state-metrics Grafana 其中 k8s-prometheus-adapter 使用 Prometheus 实现了 metrics.k8s.io 和 custom.metrics.k8s.io API，所以不需要再部署 metrics-server。 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:4:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"下载和安装 cd /opt/k8s/work git clone https://github.com/coreos/kube-prometheus.git cd kube-prometheus/ sed -i -e 's_quay.io_quay.mirrors.ustc.edu.cn_' manifests/*.yaml manifests/setup/*.yaml # 使用中科大的 Registry kubectl apply -f manifests/setup # 安装 prometheus-operator kubectl apply -f manifests/ # 安装 promethes metric adapter ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:4:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"查看运行结果 $ kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE alertmanager-main-0 2/2 Running 0 14h alertmanager-main-1 2/2 Running 0 14h alertmanager-main-2 2/2 Running 0 14h grafana-85c89999cb-cxzsd 1/1 Running 0 14h kube-state-metrics-79f47cd6dc-8h92p 3/3 Running 0 14h node-exporter-b584n 2/2 Running 0 14h node-exporter-jmcbk 2/2 Running 0 14h node-exporter-zwc8h 2/2 Running 0 14h prometheus-adapter-b8d458474-r4brv 1/1 Running 0 14h prometheus-k8s-0 3/3 Running 0 14h prometheus-k8s-1 3/3 Running 0 14h prometheus-operator-784c457694-k9s7q 2/2 Running 0 14h $ kubectl top pods -n monitoring NAME CPU(cores) MEMORY(bytes) alertmanager-main-0 2m 43Mi alertmanager-main-1 2m 41Mi alertmanager-main-2 1m 45Mi grafana-85c89999cb-cxzsd 8m 44Mi kube-state-metrics-79f47cd6dc-8h92p 0m 65Mi node-exporter-b584n 2m 34Mi node-exporter-jmcbk 4m 37Mi node-exporter-zwc8h 3m 38Mi prometheus-adapter-b8d458474-r4brv 2m 38Mi prometheus-k8s-0 68m 489Mi prometheus-k8s-1 51m 490Mi prometheus-operator-784c457694-k9s7q 0m 51Mi ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:4:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"访问测试 启动代理，访问 Prometheus port-forward 依赖 socat 访问 http://192.168.7.200:9090/new/graph?g0.expr=\u0026g0.tab=1\u0026g0.stacked=0\u0026g0.range_input=1h kubectl port-forward --address 0.0.0.0 pod/prometheus-k8s-0 -n monitoring 9090:9090 启动代理，访问 Grafana 访问 http://192.168.7.200:3000/ 默认账号密码 admin/admin，登录后需要修改密码 kubectl port-forward --address 0.0.0.0 svc/grafana -n monitoring 3000:3000 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:4:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"部署 EFK 插件 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:5:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"修改配置文件 EFK 目录是 kubernetes/cluster/addons/fluentd-elasticsearch cd /opt/k8s/work/kubernetes/cluster/addons/fluentd-elasticsearch sed -i -e 's_quay.io_quay.mirrors.ustc.edu.cn_' es-statefulset.yaml # 使用中科大的 Registry sed -i -e 's_quay.io_quay.mirrors.ustc.edu.cn_' fluentd-es-ds.yaml # 使用中科大的 Registry ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:5:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"执行定义文件 cd /opt/k8s/work/kubernetes/cluster/addons/fluentd-elasticsearch kubectl apply -f . ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:5:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"结果检查 $ kubectl get all -n kube-system |grep -E 'elasticsearch|fluentd|kibana' pod/elasticsearch-logging-0 1/1 Running 0 57m pod/elasticsearch-logging-1 1/1 Running 0 27m pod/fluentd-es-v2.7.0-8d2d8 1/1 Running 0 57m pod/fluentd-es-v2.7.0-bt9rk 1/1 Running 0 57m pod/fluentd-es-v2.7.0-r2h79 1/1 Running 0 57m pod/kibana-logging-75888755d6-vrdsc 0/1 Running 0 57m service/elasticsearch-logging ClusterIP 10.254.198.164 \u003cnone\u003e 9200/TCP 57m service/kibana-logging ClusterIP 10.254.201.178 \u003cnone\u003e 5601/TCP 57m daemonset.apps/fluentd-es-v2.7.0 3 3 3 3 3 \u003cnone\u003e 57m deployment.apps/kibana-logging 0/1 1 0 57m replicaset.apps/kibana-logging-75888755d6 1 1 0 57m statefulset.apps/elasticsearch-logging 2/2 57m kibana Pod 第一次启动时会用**较长时间(0-20分钟)**来优化和 Cache 状态页面，可以 tailf 该 Pod 的日志观察进度： kubectl logs kibana-logging-75888755d6-vrdsc -n kube-system -f 注意：只有当 Kibana pod 启动完成后，浏览器才能查看 kibana dashboard，否则会被拒绝。 ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:5:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"访问测试 访问 http://192.168.7.200:8086/api/v1/namespaces/kube-system/services/kibana-logging/proxy kubectl proxy --address='192.168.7.200' --port=8086 --accept-hosts='^*$' 在 Management -\u003e Indices 页面创建一个 index（相当于 mysql 中的一个 database），选中 Index contains time-based events，使用默认的 logstash-* pattern，点击 Create ; 创建 Index 后，稍等几分钟就可以在 Discover 菜单下看到 ElasticSearch logging 中汇聚的日志； ","date":"2019-12-10","objectID":"/k8s-install-hardway-4/:5:4","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part4","uri":"/k8s-install-hardway-4/"},{"categories":["Kubernetes"],"content":"前言 ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:1:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"系列目录 二进制部署Kubernetes Part1 二进制部署Kubernetes Part2 二进制部署Kubernetes Part3(本篇) 二进制部署Kubernetes Part4 ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:1:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"注意点 本章节所有操作默认均在 k8s-node01下操作 ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:1:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"master节点部署 master 节点运行组件需要 kube-apiserver kube-scheduler kube-controller-manager kube-apiserver，kube-scheduler 与 kube-controller-manager 均以多实例模式运行 kube-scheduler 与 kube-controller-manager 会自动选举产生一个 leader 实例，其他实例处于阻塞状态；leader 挂掉以后，重新选举产生新的 leader，保证服务的可用性 kube-apiserver 是无状态的，可以通过 kube-nginx 进行代理访问，保证服务可用性 ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:2:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"下载二进制并分发 cd /opt/k8s/work wget https://dl.k8s.io/v1.16.6/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes tar -xzvf kubernetes-src.tar.gz 拷贝二进制文件到所有 master节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp kubernetes/server/bin/{apiextensions-apiserver,kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,kubeadm,kubectl,kubelet,mounter} root@${node_ip}:/opt/k8s/bin/ ssh root@${node_ip} \"chmod +x /opt/k8s/bin/*\" done ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:2:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"kube-apiserver 集群部署 创建 kubernetes-master证书和私钥 创建证书请求 hosts字段指定授权使用该证书的 IP 和 域名列表 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e kubernetes-csr.json \u003c\u003cEOF { \"CN\": \"kubernetes-master\", \"hosts\": [ \"127.0.0.1\", \"192.168.7.200\", \"192.168.7.201\", \"192.168.7.202\", \"${CLUSTER_KUBERNETES_SVC_IP}\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local.\", \"kubernetes.default.svc.${CLUSTER_DNS_DOMAIN}.\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"opsnull\" } ] } EOF 生成证书和私钥 cfssl gencert -ca=/opt/k8s/work/ca.pem \\ -ca-key=/opt/k8s/work/ca-key.pem \\ -config=/opt/k8s/work/ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes ls kubernetes*pem 将生成的证书和私钥文件拷贝到所有 master 节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p /etc/kubernetes/cert\" scp kubernetes*.pem root@${node_ip}:/etc/kubernetes/cert/ done 创建加密配置文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e encryption-config.yaml \u003c\u003cEOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF 将加密配置文件拷贝到 master 节点的 /etc/kubernetes 目录下： cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp encryption-config.yaml root@${node_ip}:/etc/kubernetes/ done 创建审计策略文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e audit-policy.yaml \u003c\u003cEOF apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: # The following requests were manually identified as high-volume and low-risk, so drop them. - level: None resources: - group: \"\" resources: - endpoints - services - services/status users: - 'system:kube-proxy' verbs: - watch - level: None resources: - group: \"\" resources: - nodes - nodes/status userGroups: - 'system:nodes' verbs: - get - level: None namespaces: - kube-system resources: - group: \"\" resources: - endpoints users: - 'system:kube-controller-manager' - 'system:kube-scheduler' - 'system:serviceaccount:kube-system:endpoint-controller' verbs: - get - update - level: None resources: - group: \"\" resources: - namespaces - namespaces/status - namespaces/finalize users: - 'system:apiserver' verbs: - get # Don't log HPA fetching metrics. - level: None resources: - group: metrics.k8s.io users: - 'system:kube-controller-manager' verbs: - get - list # Don't log these read-only URLs. - level: None nonResourceURLs: - '/healthz*' - /version - '/swagger*' # Don't log events requests. - level: None resources: - group: \"\" resources: - events # node and pod status calls from nodes are high-volume and can be large, don't log responses # for expected updates from nodes - level: Request omitStages: - RequestReceived resources: - group: \"\" resources: - nodes/status - pods/status users: - kubelet - 'system:node-problem-detector' - 'system:serviceaccount:kube-system:node-problem-detector' verbs: - update - patch - level: Request omitStages: - RequestReceived resources: - group: \"\" resources: - nodes/status - pods/status userGroups: - 'system:nodes' verbs: - update - patch # deletecollection calls can be large, don't log responses for expected namespace deletions - level: Request omitStages: - RequestReceived users: - 'system:serviceaccount:kube-system:namespace-controller' verbs: - deletecollection # Secrets, ConfigMaps, and TokenReviews can contain sensitive \u0026 binary data, # so only log at the Metadata level. - level: Metadata omitStages: - RequestReceived resources: - group: \"\" resources: - secrets - configmaps - group: authentication.k8s.io resources: - tokenreviews # Get repsonses can be large; skip them. - level: Request omitStages: - RequestReceived resources: - group: \"\" - group: admissionregistration.k8s.io - group: apiextensions.k8s.io - group: apiregistration.k8s.io -","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:2:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"kube-controller-manager 部署 kube-controller-manager 两种情况下使用证书进行通信 与 kube-apiserver 的安全端口通信 在安全端口（https：10252）输出 prometheus 格式的 metrics 创建kube-controller-manager 证书和私钥 创建证书签名请求 hosts 列表包含所有 kube-controller-manager 节点 IP； CN 和 O 均为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。 cd /opt/k8s/work cat \u003e kube-controller-manager-csr.json \u003c\u003cEOF { \"CN\": \"system:kube-controller-manager\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"hosts\": [ \"127.0.0.1\", \"192.168.7.200\", \"192.168.7.201\", \"192.168.7.202\" ], \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:kube-controller-manager\", \"OU\": \"opsnull\" } ] } EOF 生成并分发 cd /opt/k8s/work cfssl gencert -ca=/opt/k8s/work/ca.pem \\ -ca-key=/opt/k8s/work/ca-key.pem \\ -config=/opt/k8s/work/ca-config.json \\ -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager ls kube-controller-manager*pem cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp kube-controller-manager*.pem root@${node_ip}:/etc/kubernetes/cert/ done 创建并分发 kubeconfig文件 kube-controller-manager 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-controller-manager 证书等信息： kube-controller-manager 与 kube-apiserver 混布，故直接通过节点 IP 访问 kube-apiserver； cd /opt/k8s/work source /opt/k8s/bin/environment.sh kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/k8s/work/ca.pem \\ --embed-certs=true \\ --server=\"https://##NODE_IP##:6443\" \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context system:kube-controller-manager \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig 分发 kubeconfig 至各个节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" sed -e \"s/##NODE_IP##/${node_ip}/\" kube-controller-manager.kubeconfig \u003e kube-controller-manager-${node_ip}.kubeconfig scp kube-controller-manager-${node_ip}.kubeconfig root@${node_ip}:/etc/kubernetes/kube-controller-manager.kubeconfig done 创建 kube-controller-manager systemd unit 模板文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e kube-controller-manager.service.template \u003c\u003cEOF [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] WorkingDirectory=${K8S_DIR}/kube-controller-manager ExecStart=/opt/k8s/bin/kube-controller-manager \\\\ --profiling \\\\ --cluster-name=kubernetes \\\\ --controllers=*,bootstrapsigner,tokencleaner \\\\ --kube-api-qps=1000 \\\\ --kube-api-burst=2000 \\\\ --leader-elect \\\\ --use-service-account-credentials\\\\ --concurrent-service-syncs=2 \\\\ --bind-address=##NODE_IP## \\\\ --secure-port=10252 \\\\ --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\\\ --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\\\ --port=0 \\\\ --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\ --client-ca-file=/etc/kubernetes/cert/ca.pem \\\\ --requestheader-allowed-names=\"aggregator\" \\\\ --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\\\ --requestheader-extra-headers-prefix=\"X-Remote-Extra-\" \\\\ --requestheader-group-headers=X-Remote-Group \\\\ --requestheader-username-headers=X-Remote-User \\\\ --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\ --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\\\ --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\\\ --experimental-cluster-signing-duration=876000h \\\\ --horizontal-pod-autoscaler-sync-period=10s \\\\ --concurrent","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:2:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"kube-scheduler 部署 创建 kube-scheduler 证书和私钥 hosts 列表包含所有 kube-scheduler 节点 IP； CN 和 O 均为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限； cd /opt/k8s/work cat \u003e kube-scheduler-csr.json \u003c\u003cEOF { \"CN\": \"system:kube-scheduler\", \"hosts\": [ \"127.0.0.1\", \"192.168.7.200\", \"192.168.7.201\", \"192.168.7.202\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:kube-scheduler\", \"OU\": \"opsnull\" } ] } EOF 生成并分发 cd /opt/k8s/work cfssl gencert -ca=/opt/k8s/work/ca.pem \\ -ca-key=/opt/k8s/work/ca-key.pem \\ -config=/opt/k8s/work/ca-config.json \\ -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler ls kube-scheduler*pem cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp kube-scheduler*.pem root@${node_ip}:/etc/kubernetes/cert/ done 创建和分发 kubeconfig 文件 kube-scheduler 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-scheduler 证书： cd /opt/k8s/work source /opt/k8s/bin/environment.sh kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/k8s/work/ca.pem \\ --embed-certs=true \\ --server=\"https://##NODE_IP##:6443\" \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context system:kube-scheduler \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig 分发至各个master节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" sed -e \"s/##NODE_IP##/${node_ip}/\" kube-scheduler.kubeconfig \u003e kube-scheduler-${node_ip}.kubeconfig scp kube-scheduler-${node_ip}.kubeconfig root@${node_ip}:/etc/kubernetes/kube-scheduler.kubeconfig done 创建 kube-scheduler 配置文件 --kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver； --leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态； cd /opt/k8s/work cat \u003ekube-scheduler.yaml.template \u003c\u003cEOF apiVersion: kubescheduler.config.k8s.io/v1alpha1 kind: KubeSchedulerConfiguration bindTimeoutSeconds: 600 clientConnection: burst: 200 kubeconfig: \"/etc/kubernetes/kube-scheduler.kubeconfig\" qps: 100 enableContentionProfiling: false enableProfiling: true hardPodAffinitySymmetricWeight: 1 healthzBindAddress: ##NODE_IP##:10251 leaderElection: leaderElect: true metricsBindAddress: ##NODE_IP##:10251 EOF 替换模板文件中的变量： cd /opt/k8s/work source /opt/k8s/bin/environment.sh for (( i=0; i \u003c 3; i++ )) do sed -e \"s/##NODE_NAME##/${NODE_NAMES[i]}/\" -e \"s/##NODE_IP##/${NODE_IPS[i]}/\" kube-scheduler.yaml.template \u003e kube-scheduler-${NODE_IPS[i]}.yaml done ls kube-scheduler*.yaml 分发至各个master节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp kube-scheduler-${node_ip}.yaml root@${node_ip}:/etc/kubernetes/kube-scheduler.yaml done 创建 kube-scheduler systemd unit 模板文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e kube-scheduler.service.template \u003c\u003cEOF [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] WorkingDirectory=${K8S_DIR}/kube-scheduler ExecStart=/opt/k8s/bin/kube-scheduler \\\\ --config=/etc/kubernetes/kube-scheduler.yaml \\\\ --bind-address=##NODE_IP## \\\\ --secure-port=10259 \\\\ --port=0 \\\\ --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\\\ --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\\\ --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\ --client-ca-file=/etc/kubernetes/cert/ca.pem \\\\ --requestheader-allowed-names=\"\" \\\\ --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\\\ --requestheader-extra-headers-prefix=\"X-Remote-Extra-\" \\\\ --requestheader-group-heade","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:2:4","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"work节点部署 work节点部署包含以下组件 containerd kubelet kube-proxy calico kube-nginx 依赖安装 source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"yum install -y epel-release\" \u0026 ssh root@${node_ip} \"yum install -y chrony conntrack ipvsadm ipset jq iptables curl sysstat libseccomp wget socat git\" \u0026 done ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:3:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"api server 高可用 使用 nginx 4 层透明代理功能实现 Kubernetes worker 节点组件高可用访问 kube-apiserver 集群 控制节点的 kube-controller-manager、kube-scheduler 是多实例部署且连接本机的 kube-apiserver，所以只要有一个实例正常，就可以保证高可用； 集群内的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的； 在每个节点起一个 nginx 进程，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡； kubelet、kube-proxy 通过本地的 nginx（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用； 下载编译安装 Nginx 下载源码 cd /opt/k8s/work wget http://nginx.org/download/nginx-1.15.3.tar.gz tar -xzvf nginx-1.15.3.tar.gz 配置编译参数 --with-stream：开启 4 层透明转发(TCP Proxy)功能； --without-xxx：关闭所有其他功能，这样生成的动态链接二进制程序依赖最小； cd /opt/k8s/work/nginx-1.15.3 mkdir nginx-prefix yum install -y gcc make ./configure --with-stream --without-http --prefix=$(pwd)/nginx-prefix --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module 安装 cd /opt/k8s/work/nginx-1.15.3 make \u0026\u0026 make install 验证编译的nginx 正常输出版本 cd /opt/k8s/work/nginx-1.15.3 ./nginx-prefix/sbin/nginx -v 安装部署 创建目录 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p /opt/k8s/kube-nginx/{conf,logs,sbin}\" done 分发二进制文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p /opt/k8s/kube-nginx/{conf,logs,sbin}\" scp /opt/k8s/work/nginx-1.15.3/nginx-prefix/sbin/nginx root@${node_ip}:/opt/k8s/kube-nginx/sbin/kube-nginx ssh root@${node_ip} \"chmod a+x /opt/k8s/kube-nginx/sbin/*\" done 配置 nginx，开启4层透明转发功能 cd /opt/k8s/work cat \u003e kube-nginx.conf \u003c\u003c \\EOF worker_processes 1; events { worker_connections 1024; } stream { upstream backend { hash $remote_addr consistent; server 192.168.7.200:6443 max_fails=3 fail_timeout=30s; server 192.168.7.201:6443 max_fails=3 fail_timeout=30s; server 192.168.7.202:6443 max_fails=3 fail_timeout=30s; } server { listen 127.0.0.1:8443; proxy_connect_timeout 1s; proxy_pass backend; } } EOF 分发配置文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp kube-nginx.conf root@${node_ip}:/opt/k8s/kube-nginx/conf/kube-nginx.conf done 配置 systemd unit 文件 与 启动服务 配置 nginx systemd unit 文件 cd /opt/k8s/work cat \u003e kube-nginx.service \u003c\u003cEOF [Unit] Description=kube-apiserver nginx proxy After=network.target After=network-online.target Wants=network-online.target [Service] Type=forking ExecStartPre=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -t ExecStart=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx ExecReload=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -s reload PrivateTmp=true Restart=always RestartSec=5 StartLimitInterval=0 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 分发配置 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp kube-nginx.service root@${node_ip}:/etc/systemd/system/ done 启动服务 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"systemctl daemon-reload \u0026\u0026 systemctl enable kube-nginx \u0026\u0026 systemctl restart kube-nginx\" done 验证 kube-nginx cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"systemctl status kube-nginx |grep 'Active:'\" done ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:3:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"部署 containerd containerd 实现了 kubernetes 的 Container Runtime Interface (CRI) 接口，提供容器运行时核心功能，如镜像管理、容器管理等，相比 dockerd 更加简单、健壮和可移植。 下载分发二进制 cd /opt/k8s/work wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.17.0/crictl-v1.17.0-linux-amd64.tar.gz \\ https://github.com/opencontainers/runc/releases/download/v1.0.0-rc10/runc.amd64 \\ https://github.com/containernetworking/plugins/releases/download/v0.8.5/cni-plugins-linux-amd64-v0.8.5.tgz \\ https://github.com/containerd/containerd/releases/download/v1.3.3/containerd-1.3.3.linux-amd64.tar.gz cd /opt/k8s/work mkdir containerd tar -xvf containerd-1.3.3.linux-amd64.tar.gz -C containerd tar -xvf crictl-v1.17.0-linux-amd64.tar.gz mkdir cni-plugins tar -xvf cni-plugins-linux-amd64-v0.8.5.tgz -C cni-plugins mv runc.amd64 runc 分发到各个节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp containerd/bin/* crictl cni-plugins/* runc root@${node_ip}:/opt/k8s/bin ssh root@${node_ip} \"chmod a+x /opt/k8s/bin/* \u0026\u0026 mkdir -p /etc/cni/net.d\" done 创建分发 containerd 配置文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003c\u003c EOF | sudo tee containerd-config.toml version = 2 root = \"${CONTAINERD_DIR}/root\" state = \"${CONTAINERD_DIR}/state\" [plugins] [plugins.\"io.containerd.grpc.v1.cri\"] sandbox_image = \"registry.cn-beijing.aliyuncs.com/images_k8s/pause-amd64:3.1\" [plugins.\"io.containerd.grpc.v1.cri\".cni] bin_dir = \"/opt/k8s/bin\" conf_dir = \"/etc/cni/net.d\" [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\" runtime_root = \"\" no_shim = false shim_debug = false [plugins.\"io.containerd.grpc.v1.cri\".registry] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [\"https://jxus37ad.mirror.aliyuncs.com\"] EOF 分发配置文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p /etc/containerd/ ${CONTAINERD_DIR}/{root,state}\" scp containerd-config.toml root@${node_ip}:/etc/containerd/config.toml done 创建 分发 containerd systemd unit 配置文件 cd /opt/k8s/work cat \u003c\u003cEOF | sudo tee containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] Environment=\"PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin\" ExecStartPre=/sbin/modprobe overlay ExecStart=/opt/k8s/bin/containerd Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF 分发配置文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp containerd.service root@${node_ip}:/etc/systemd/system ssh root@${node_ip} \"systemctl enable containerd \u0026\u0026 systemctl restart containerd\" done 创建分发 crictl 配置文件 crictl 是兼容 CRI 容器运行时的命令行工具，提供类似于 docker 命令的功能 cd /opt/k8s/work cat \u003c\u003c EOF | sudo tee crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF 分发到 worker 节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp crictl.yaml root@${node_ip}:/etc/crictl.yaml done ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:3:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"部署 kubelet kubelet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。 kubelet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。 为确保安全，部署时关闭了 kubelet 的非安全 http 端口，对请求进行认证和授权，拒绝未授权的访问(如 apiserver、heapster 的请求)。 下载分发 在 master 节点部署的时候，已经分发到各个节点了 创建 kubelet bootstrap kubeconfig 文件 向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书； cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_name in ${NODE_NAMES[@]} do echo \"\u003e\u003e\u003e ${node_name}\" # 创建 token export BOOTSTRAP_TOKEN=$(kubeadm token create \\ --description kubelet-bootstrap-token \\ --groups system:bootstrappers:${node_name} \\ --kubeconfig ~/.kube/config) # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/cert/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig # 设置客户端认证参数 kubectl config set-credentials kubelet-bootstrap \\ --token=${BOOTSTRAP_TOKEN} \\ --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig done 查看 kubeadm 为各节点创建的 token kubeadm token list --kubeconfig ~/.kube/config token 的有效期为 24h，超期后不能用来进行 boostrap kubelet，且会被 kube-controller-manager 的 tokencleaner 清理； kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 system:bootstrap:\u003cToken ID\u003e，group 设置为 system:bootstrappers，后续将为这个 group 设置 ClusterRoleBinding； 分发 bootstrap kubeconfig 文件到所有 worker 节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_name in ${NODE_NAMES[@]} do echo \"\u003e\u003e\u003e ${node_name}\" scp kubelet-bootstrap-${node_name}.kubeconfig root@${node_name}:/etc/kubernetes/kubelet-bootstrap.kubeconfig done 创建分发kubelet 参数配置文件 从 v1.10 开始，部分 kubelet 参数需在配置文件中配置，kubelet --help 会提示： DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag 创建 kubelet 参数配置文件模板 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e kubelet-config.yaml.template \u003c\u003cEOF kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 address: \"##NODE_IP##\" staticPodPath: \"\" syncFrequency: 1m fileCheckFrequency: 20s httpCheckFrequency: 20s staticPodURL: \"\" port: 10250 readOnlyPort: 0 rotateCertificates: true serverTLSBootstrap: true authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \"/etc/kubernetes/cert/ca.pem\" authorization: mode: Webhook registryPullQPS: 0 registryBurst: 20 eventRecordQPS: 0 eventBurst: 20 enableDebuggingHandlers: true enableContentionProfiling: true healthzPort: 10248 healthzBindAddress: \"##NODE_IP##\" clusterDomain: \"${CLUSTER_DNS_DOMAIN}\" clusterDNS: - \"${CLUSTER_DNS_SVC_IP}\" nodeStatusUpdateFrequency: 10s nodeStatusReportFrequency: 1m imageMinimumGCAge: 2m imageGCHighThresholdPercent: 85 imageGCLowThresholdPercent: 80 volumeStatsAggPeriod: 1m kubeletCgroups: \"\" systemCgroups: \"\" cgroupRoot: \"\" cgroupsPerQOS: true cgroupDriver: cgroupfs runtimeRequestTimeout: 10m hairpinMode: promiscuous-bridge maxPods: 220 podCIDR: \"${CLUSTER_CIDR}\" podPidsLimit: -1 resolvConf: /etc/resolv.conf maxOpenFiles: 1000000 kubeAPIQPS: 1000 kubeAPIBurst: 2000 serializeImagePulls: false evictionHard: memory.available: \"100Mi\" nodefs.available: \"10%\" nodefs.inodesFree: \"5%\" imagefs.available: \"15%\" evictionSoft: {} enableControllerAttachDetach: true failSwapOn: true containerLogMaxSize: 20Mi containerLogMaxFiles: 10 systemReserved: {} kubeReserved: {} systemReservedCgroup: \"\" kubeReservedCgroup: \"\" enforceNodeAllocatable: [\"pods\"] EOF 字段 含义 address kubelet 安全端口（https，10250）监听的地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API； readOnlyPort=0 关闭只读端口(默认 10255)，等效为未指定 authentication.anonymous.enabled 设置为 false，不允许匿名访问 10250 端口 authentication.x509.clientCAFile 指定签名客户端证书的 CA 证书，开启 HTTP 证书认证； authentication.webhook.ena","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:3:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"部署 kube-proxy 组件 kube-proxy 运行在所有 worker 节点上，它监听 apiserver 中 service 和 endpoint 的变化情况，创建路由规则以提供服务 IP 和负载均衡功能。 本文档讲解部署 ipvs 模式的 kube-proxy 过程。 创建 kube-proxy 证书 CN：指定该证书的 User 为 system:kube-proxy； 预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限； 该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空； cd /opt/k8s/work cat \u003e kube-proxy-csr.json \u003c\u003cEOF { \"CN\": \"system:kube-proxy\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"opsnull\" } ] } EOF 生成证书和私钥 cd /opt/k8s/work cfssl gencert -ca=/opt/k8s/work/ca.pem \\ -ca-key=/opt/k8s/work/ca-key.pem \\ -config=/opt/k8s/work/ca-config.json \\ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy ls kube-proxy* 创建和分发 kubeconfig 文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/k8s/work/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 分发 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_name in ${NODE_NAMES[@]} do echo \"\u003e\u003e\u003e ${node_name}\" scp kube-proxy.kubeconfig root@${node_name}:/etc/kubernetes/ done 创建 kube-proxy 配置文件 从 v1.10 开始，kube-proxy 部分参数可以配置文件中配置。可以使用 --write-config-to 选项生成该配置文件，或者参考 源代码的注释。 创建 kube-proxy config 文件模板： bindAddress: 监听地址； clientConnection.kubeconfig: 连接 apiserver 的 kubeconfig 文件； clusterCIDR: kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT； hostnameOverride: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则； mode: 使用 ipvs 模式； cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e kube-proxy-config.yaml.template \u003c\u003cEOF kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: burst: 200 kubeconfig: \"/etc/kubernetes/kube-proxy.kubeconfig\" qps: 100 bindAddress: ##NODE_IP## healthzBindAddress: ##NODE_IP##:10256 metricsBindAddress: ##NODE_IP##:10249 enableProfiling: true clusterCIDR: ${CLUSTER_CIDR} hostnameOverride: ##NODE_NAME## mode: \"ipvs\" portRange: \"\" iptables: masqueradeAll: false ipvs: scheduler: rr excludeCIDRs: [] EOF 分发至各个节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for (( i=0; i \u003c 3; i++ )) do echo \"\u003e\u003e\u003e ${NODE_NAMES[i]}\" sed -e \"s/##NODE_NAME##/${NODE_NAMES[i]}/\" -e \"s/##NODE_IP##/${NODE_IPS[i]}/\" kube-proxy-config.yaml.template \u003e kube-proxy-config-${NODE_NAMES[i]}.yaml.template scp kube-proxy-config-${NODE_NAMES[i]}.yaml.template root@${NODE_NAMES[i]}:/etc/kubernetes/kube-proxy-config.yaml done 创建和分发 kube-proxy systemd unit 文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e kube-proxy.service \u003c\u003cEOF [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=${K8S_DIR}/kube-proxy ExecStart=/opt/k8s/bin/kube-proxy \\\\ --config=/etc/kubernetes/kube-proxy-config.yaml \\\\ --logtostderr=true \\\\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_name in ${NODE_NAMES[@]} do echo \"\u003e\u003e\u003e ${node_name}\" scp kube-proxy.service root@${node_name}:/etc/systemd/system/ done 启动 kube-proxy 服务 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p ${K8S_DIR}/kube-proxy\" ssh root@${node_ip} \"modprobe ip_vs_rr\" ssh root@${node_ip} \"systemctl daemon-reload \u0026\u0026 systemctl enable kube-proxy \u0026\u0026 systemctl restart kube-proxy\" done 检查","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:3:4","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"部署Calico网络组件 calico 使用 IPIP 或 BGP 技术（默认为 IPIP）为各节点创建一个可以互通的 Pod 网络。 安装部署 cd /opt/k8s/work curl https://docs.projectcalico.org/manifests/calico.yaml -O 修改配置： calico 自动探查互联网卡，如果有多网卡，则可以配置用于互联的网络接口命名正则表达式，如上面的 ens.*(根据自己服务器的网络接口名修改)； cp calico.yaml calico.yaml.orig vim calico.yaml diff calico.yaml.orig calico.yaml \u003c # - name: CALICO_IPV4POOL_CIDR \u003c # value: \"192.168.0.0/16\" --- \u003e - name: CALICO_IPV4POOL_CIDR \u003e value: \"172.30.0.0/16\" \u003e - name: IP_AUTODETECTION_METHOD \u003e value: \"interface=ens.*\" 3649c3651 \u003c path: /opt/cni/bin --- \u003e path: /opt/k8s/bin 部署 calico calico 插架以 daemonset 方式运行在所有的 K8S 节点上 kubectl apply -f calico.yaml 检查 calico kubectl get pods -n kube-system -o wide 使用 crictl 命令查看 calico 使用的镜像 crictl images 如果 crictl 输出为空或执行失败，则有可能是缺少配置文件 /etc/crictl.yaml 导致的，该文件的配置如下： cat /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:3:5","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"验证功能 ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:4:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"检查节点功能 都为 Ready 且版本为 v1.16.6 时正常。 kubectl get nodes ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:4:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"创建测试文件 cd /opt/k8s/work cat \u003e nginx-ds.yml \u003c\u003cEOF apiVersion: v1 kind: Service metadata: name: nginx-ds labels: app: nginx-ds spec: type: NodePort selector: app: nginx-ds ports: - name: http port: 80 targetPort: 80 --- apiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ds labels: addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: app: nginx-ds template: metadata: labels: app: nginx-ds spec: containers: - name: my-nginx image: nginx:1.7.9 ports: - containerPort: 80 EOF ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:4:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"执行测试 kubectl create -f nginx-ds.yml ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:4:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"检查节点 POD IP 连通性 kubectl get pods -o wide -l app=nginx-ds NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-ds-5qszv 1/1 Running 0 4h25m 172.30.135.129 k8s-node03 \u003cnone\u003e \u003cnone\u003e nginx-ds-n5tzp 1/1 Running 0 4h25m 172.30.85.193 k8s-node01 \u003cnone\u003e \u003cnone\u003e nginx-ds-pdsp2 1/1 Running 0 4h25m 172.30.58.194 k8s-node02 \u003cnone\u003e \u003cnone\u003e 在所有的 Node上对 Pod IP 进行联通性测试 source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh ${node_ip} \"ping -c 1 172.30.135.129\" ssh ${node_ip} \"ping -c 1 172.30.85.193\" ssh ${node_ip} \"ping -c 1 172.30.58.194\" done ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:4:4","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"检查服务IP 和 端口可达性 kubectl get svc -l app=nginx-ds NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-ds NodePort 10.254.217.207 \u003cnone\u003e 80:32716/TCP 4h30m 通过信息可以看出 Service Cluster IP：10.254.217.207 服务端口：80 NodePort：32716 source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh ${node_ip} \"curl -s 10.254.217.207\" done 预期打印 Nginx 信息 ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:4:5","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"检查服务 NodePort 可达性 source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh ${node_ip} \"curl -s ${node_ip}:32716\" done 预期打印 Nginx 信息 ","date":"2019-12-10","objectID":"/k8s-install-hardway-3/:4:6","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part3","uri":"/k8s-install-hardway-3/"},{"categories":["Kubernetes"],"content":"前言 ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:1:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"系列目录 二进制部署Kubernetes Part1 二进制部署Kubernetes Part2(本篇) 二进制部署Kubernetes Part3 二进制部署Kubernetes Part4 ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:1:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"注意 本章节所有操作默认均在 k8s-node01下操作 kubeconfig 文件通用， 存放一般在需要使用kubectl机器的 ~/.kube/config 位置 kubeconfig 是与 APIServer 交互的凭证，基于 client-go 对 Kubernetes 的二次开发也是通过该配置文件接入 ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:1:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"部署配置 kubectl ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:2:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"下载分发二进制文件 cd /opt/k8s/work wget https://dl.k8s.io/v1.16.6/kubernetes-client-linux-amd64.tar.gz tar -xzvf kubernetes-client-linux-amd64.tar.gz 分发至各个节点，如果只需要在主节点使用，可以不用分发 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp kubernetes/client/bin/kubectl root@${node_ip}:/opt/k8s/bin/ ssh root@${node_ip} \"chmod +x /opt/k8s/bin/*\" done ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:2:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"创建 admin证书和密钥 kubectl 通过 https 与 kube-apiserver 进行安全通信;kube-apiserver对于 kubectl 请求包含的证书进行认证和授权 kubectl 用于整个集群的管理，所以创建最高权限的 admin 证书 cd /opt/k8s/work cat \u003e admin-csr.json \u003c\u003cEOF { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"opsnull\" } ] } EOF O: system:masters：kube-apiserver 收到使用该证书的客户端请求后，为请求添加组（Group）认证标识 system:masters； 预定义的 ClusterRoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予操作集群所需的最高权限； 该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空； 生成证书 cd /opt/k8s/work cfssl gencert -ca=/opt/k8s/work/ca.pem \\ -ca-key=/opt/k8s/work/ca-key.pem \\ -config=/opt/k8s/work/ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare admin 忽略警告消息 [WARNING] This certificate lacks a \"hosts\" field.； ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:2:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"创建并分发 kubeconfig 文件 kubeconfig 包含 kube-apiserver 地址与认证信息， cd /opt/k8s/work source /opt/k8s/bin/environment.sh # 设置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/k8s/work/ca.pem \\ --embed-certs=true \\ --server=https://${NODE_IPS[0]}:6443 \\ --kubeconfig=kubectl.kubeconfig # 设置客户端认证参数 kubectl config set-credentials admin \\ --client-certificate=/opt/k8s/work/admin.pem \\ --client-key=/opt/k8s/work/admin-key.pem \\ --embed-certs=true \\ --kubeconfig=kubectl.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=kubectl.kubeconfig # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig --certificate-authority：验证 kube-apiserver 证书的根证书； --client-certificate、--client-key：刚生成的 admin 证书和私钥，与 kube-apiserver https 通信时使用； --embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(否则，写入的是证书文件路径，后续拷贝 kubeconfig 到其它机器时，还需要单独拷贝证书文件，不方便。)； --server：指定 kube-apiserver 的地址，这里指向第一个节点上的服务； 分发到需要使用 kubectl 的节点， 如果只需要主节点，则不需要 分发配置文件到在主机的 ~/.kube/config 下 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p ~/.kube\" scp kubectl.kubeconfig root@${node_ip}:~/.kube/config done ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:2:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"部署 etcd 集群 本文档构建三个节点高可用的 etcd 集群 注意： flanneld 插件与 etcd v3.4.x 版本不兼容，如果需要部署 flanneld 插件需要使用 etcd v3.3.x ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:3:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"下载分发 etcd 二进制文件 cd /opt/k8s/work wget https://github.com/coreos/etcd/releases/download/v3.4.3/etcd-v3.4.3-linux-amd64.tar.gz tar -xvf etcd-v3.4.3-linux-amd64.tar.gz cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp etcd-v3.4.3-linux-amd64/etcd* root@${node_ip}:/opt/k8s/bin ssh root@${node_ip} \"chmod +x /opt/k8s/bin/*\" done ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:3:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"创建 etcd 证书和私钥 以 k8s User Name 创建证书签名请求 hosts 中需要包含 etcd 集群中所有成员地址 cd /opt/k8s/work cat \u003e etcd-csr.json \u003c\u003cEOF { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"192.168.7.200\", \"192.168.7.201\", \"192.168.7.202\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"opsnull\" } ] } EOF 生成证书公钥以及私钥 cd /opt/k8s/work cfssl gencert -ca=/opt/k8s/work/ca.pem \\ -ca-key=/opt/k8s/work/ca-key.pem \\ -config=/opt/k8s/work/ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd ls etcd*pem 分发到集群各个节点 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p /etc/etcd/cert\" scp etcd*.pem root@${node_ip}:/etc/etcd/cert/ done ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:3:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"创建 etcd 的 systemd unit 模板文件 注意：此处对于模板文件的生成 ${} 引用环境变量会直接输出值到文件 如果环境变量值存在错误，模板文件会存在问题，需要重新回到此步生成模板文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh cat \u003e etcd.service.template \u003c\u003cEOF [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=${ETCD_DATA_DIR} ExecStart=/opt/k8s/bin/etcd \\\\ --data-dir=${ETCD_DATA_DIR} \\\\ --wal-dir=${ETCD_WAL_DIR} \\\\ --name=##NODE_NAME## \\\\ --cert-file=/etc/etcd/cert/etcd.pem \\\\ --key-file=/etc/etcd/cert/etcd-key.pem \\\\ --trusted-ca-file=/etc/kubernetes/cert/ca.pem \\\\ --peer-cert-file=/etc/etcd/cert/etcd.pem \\\\ --peer-key-file=/etc/etcd/cert/etcd-key.pem \\\\ --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --listen-peer-urls=https://##NODE_IP##:2380 \\\\ --initial-advertise-peer-urls=https://##NODE_IP##:2380 \\\\ --listen-client-urls=https://##NODE_IP##:2379,http://127.0.0.1:2379 \\\\ --advertise-client-urls=https://##NODE_IP##:2379 \\\\ --initial-cluster-token=etcd-cluster-0 \\\\ --initial-cluster=${ETCD_NODES} \\\\ --initial-cluster-state=new \\\\ --auto-compaction-mode=periodic \\\\ --auto-compaction-retention=1 \\\\ --max-request-bytes=33554432 \\\\ --quota-backend-bytes=6442450944 \\\\ --heartbeat-interval=250 \\\\ --election-timeout=2000 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target EOF 字段 含义 Working Directory，–data-dir 代指工作目录与数据目录为 ${ETCD_DATA_DIR}，需要启动之前创建目录 –wal-dir 指定 wal 目录，为了提高性能，一般使用 SSD 或者是与 --data-dir不同的磁盘 –name 指定节点名称，当 --initial-cluster-state 值为 new 时，--name 的参数值必须位于 --initial-cluster 列表中； –cert-file, –key-file etcd server 与 client 通信时使用的证书与私钥 –trusted-ca-file 签名 client 证书的 CA 证书，用于验证 client 的证书 –peer-cert-file, –peer-key-file etcd 与 peer 通信使用的证书和密钥 –peer-trusted-ca-file 签名 peer 证书的 CA 证书，用于验证 peer 证书 ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:3:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"为各节点创建分发 etcd systemd unit 文件 替换模板文件中的变量, 创建各节点的 systemd unit 文件 NODE_NAMES 和NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP； cd /opt/k8s/work source /opt/k8s/bin/environment.sh for (( i=0; i \u003c 3; i++ )) do sed -e \"s/##NODE_NAME##/${NODE_NAMES[i]}/\" -e \"s/##NODE_IP##/${NODE_IPS[i]}/\" etcd.service.template \u003e etcd-${NODE_IPS[i]}.service done ls *.service 分发生成的 systemd unit 文件 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp etcd-${node_ip}.service root@${node_ip}:/etc/systemd/system/etcd.service done ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:3:4","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"启动 etcd 启动服务之前必须要创建 etcd 数据目录 与 工作目录 etcd 首次启动会等待其他 etcd 节点加入集群， systemctl start etcd 卡住一段时间，属于正常现象 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p ${ETCD_DATA_DIR}${ETCD_WAL_DIR}\" ssh root@${node_ip} \"systemctl daemon-reload \u0026\u0026 systemctl enable etcd \u0026\u0026 systemctl restart etcd \" \u0026 done ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:3:5","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"检查启动并验证服务状态 3.4.3 版本的 etcd/etcdctl 默认启用了 V3 API，所以执行 etcdctl 命令时不需要再指定环境变量 ETCDCTL_API=3； 从 K8S 1.13 开始，不再支持 v2 版本的 etcd； cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" /opt/k8s/bin/etcdctl \\ --endpoints=https://${node_ip}:2379 \\ --cacert=/etc/kubernetes/cert/ca.pem \\ --cert=/etc/etcd/cert/etcd.pem \\ --key=/etc/etcd/cert/etcd-key.pem endpoint health done 预期输出 \u003e\u003e\u003e 192.168.7.200 https://192.168.7.200:2379 is healthy: successfully committed proposal: took = 2.439169ms \u003e\u003e\u003e 192.168.7.201 https://192.168.7.201:2379 is healthy: successfully committed proposal: took = 2.030526ms \u003e\u003e\u003e 192.168.7.202 https://192.168.7.202:2379 is healthy: successfully committed proposal: took = 2.46203ms ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:3:6","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"查看当前 Leader 通过 IS LEADER 字段可以判断出当前哪个节点是 LEADER 节点 source /opt/k8s/bin/environment.sh /opt/k8s/bin/etcdctl \\ -w table --cacert=/etc/kubernetes/cert/ca.pem \\ --cert=/etc/etcd/cert/etcd.pem \\ --key=/etc/etcd/cert/etcd-key.pem \\ --endpoints=${ETCD_ENDPOINTS} endpoint status +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://192.168.7.200:2379 | 4e420e95e8f0acdf | 3.4.3 | 16 kB | true | false | 2 | 8 | 8 | | | https://192.168.7.201:2379 | 90c0bee619181d8b | 3.4.3 | 16 kB | false | false | 2 | 8 | 8 | | | https://192.168.7.202:2379 | 9a039738e256b955 | 3.4.3 | 16 kB | false | false | 2 | 8 | 8 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ ","date":"2019-12-09","objectID":"/k8s-install-hardway-2/:3:7","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part2","uri":"/k8s-install-hardway-2/"},{"categories":["Kubernetes"],"content":"前言 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:1:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"参考地址 本次安装完全参照于 follow-me-install-kubernetes-cluster 项目 对于源项目操作进行了部分扩展的解释与优化 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:1:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"版本信息 当前版本基于 v1.16.x 版本 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:1:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"系列目录 二进制部署Kubernetes Part1(本篇) 二进制部署Kubernetes Part2 二进制部署Kubernetes Part3 二进制部署Kubernetes Part4 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:1:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"环境准备 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:2:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"版本信息 组件 版本 发布时间 kubernetes 1.16.6 2020-01-22 etcd 3.4.3 2019-10-24 containerd 1.3.3 2020-02-07 runc 1.0.0-rc10 2019-12-23 calico 3.12.0 2020-01-27 coredns 1.6.6 2019-12-20 dashboard v2.0.0-rc4 2020-02-06 k8s-prometheus-adapter 0.5.0 2019-04-03 prometheus-operator 0.35.0 2020-01-13 prometheus 2.15.2 2020-01-06 elasticsearch、kibana 7.2.0 2019-06-25 cni-plugins 0.8.5 2019-12-20 metrics-server 0.3.6 2019-10-15 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:2:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"主要配置策略 kube-apiserver： 使用节点本地 nginx 4 层透明代理实现高可用； 关闭非安全端口 8080 和匿名访问； 在安全端口 6443 接收 https 请求； 严格的认证和授权策略 (x509、token、RBAC)； 开启 bootstrap token 认证，支持 kubelet TLS bootstrapping； 使用 https 访问 kubelet、etcd，加密通信； kube-controller-manager： 3 节点高可用； 关闭非安全端口，在安全端口 10252 接收 https 请求； 使用 kubeconfig 访问 apiserver 的安全端口； 自动 approve kubelet 证书签名请求 (CSR)，证书过期后自动轮转； 各 controller 使用自己的 ServiceAccount 访问 apiserver； kube-scheduler： 3 节点高可用； 使用 kubeconfig 访问 apiserver 的安全端口； kubelet： 使用 kubeadm 动态创建 bootstrap token，而不是在 apiserver 中静态配置； 使用 TLS bootstrap 机制自动生成 client 和 server 证书，过期后自动轮转； 在 KubeletConfiguration 类型的 JSON 文件配置主要参数； 关闭只读端口，在安全端口 10250 接收 https 请求，对请求进行认证和授权，拒绝匿名访问和非授权访问； 使用 kubeconfig 访问 apiserver 的安全端口； kube-proxy： 使用 kubeconfig 访问 apiserver 的安全端口； 在 KubeProxyConfiguration 类型的 JSON 文件配置主要参数； 使用 ipvs 代理模式； 集群插件： DNS：使用功能、性能更好的 coredns； Dashboard：支持登录认证； Metric：metrics-server，使用 https 访问 kubelet 安全端口； Log：Elasticsearch、Fluend、Kibana； Registry 镜像库：docker-registry、harbor； ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:3:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"机器信息 IP 主机名 备注 192.168.7.200 k8s-node01 192.168.7.201 k8s-node02 192.168.7.203 k8s-node03 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:3:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"基础配置 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"注意事项 基础配置章节，如果没有特别指明在 k8s-node01 节点，所有操作所有节点都需要进行 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"主机名 修改机器主机名 hostnamectl set-hostname k8s-node01 hostnamectl set-hostname k8s-node02 hostnamectl set-hostname k8s-node03 修改 hosts 信息，关联节点DNS解析 cat \u003e\u003e /etc/hosts \u003c\u003cEOF 192.168.7.200 k8s-node01 192.168.7.201 k8s-node02 192.168.7.202 k8s-node03 EOF 重新连接当前 SSH 连接，主机名显示生效 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"节点免密 在 k8s-node01节点执行 保证 k8s-node01 可以免密登录到其他机器 ssh-keygen -t rsa ssh-copy-id root@k8s-node01 ssh-copy-id root@k8s-node02 ssh-copy-id root@k8s-node03 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"添加本次二进制程序存放路径到Path 以 /opt/k8s 作为部署根目录，二进制文件存放在 bin 中，添加到 Path 具体目录后续会创建 echo 'PATH=/opt/k8s/bin:$PATH' \u003e\u003e/root/.bashrc source /root/.bashrc ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:4","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"依赖安装 kube-proxy 使用 ipvs 模式安装， ipvsadm 为 ipvs 的管理工具 etcd 集群节点之间需要时间同步，chrony 用于系统时间同步 yum install -y epel-release yum install -y chrony conntrack ipvsadm ipset jq iptables curl sysstat libseccomp wget socat git ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:5","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"防火墙配置 关闭防火墙 清空 iptables 规则 设置默认转发策略 systemctl stop firewalld \u0026\u0026 systemctl disable firewalld iptables -F \u0026\u0026 iptables -X \u0026\u0026 iptables -F -t nat \u0026\u0026 iptables -X -t nat iptables -P FORWARD ACCEPT ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:6","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"关闭SWAP分区 关闭swap分区，否则可能会影响 kubelet 启动 同样可以通过设置 kubelet 启动参数 –fail-swap-on 为 false 关闭 swap 检查 swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:7","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"关闭 SELINUX 关闭 SELINUX 否则可能 kubelet 挂载目录报错：Permission denied setenforce 0 sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:8","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"内核调优 关闭 tcp_tw_recycle，否则与 NAT 冲突， 可能导致服务不通； cat \u003e /etc/sysctl.d/kubernetes.conf \u003c\u003cEOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 net.ipv4.tcp_tw_recycle=0 net.ipv4.neigh.default.gc_thresh1=1024 net.ipv4.neigh.default.gc_thresh1=2048 net.ipv4.neigh.default.gc_thresh1=4096 vm.swappiness=0 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_instances=8192 fs.inotify.max_user_watches=1048576 fs.file-max=52706963 fs.nr_open=52706963 net.ipv6.conf.all.disable_ipv6=1 net.netfilter.nf_conntrack_max=2310720 EOF 否则会执行 调优参数 生效的时候报错：sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory modprobe br_netfilter sysctl -p /etc/sysctl.d/kubernetes.conf ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:9","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"设置时区以及同步 timedatectl set-timezone Asia/Shanghai systemctl enable chronyd \u0026\u0026 systemctl start chronyd 检查状态 timedatectl status 如果输出内容存在 即可 Time zone: Asia/Shanghai (CST, +0800) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no 将当前 UTC 时间写入硬件时钟，并重启系统时间相关的依赖服务 timedatectl set-local-rtc 0 systemctl restart rsyslog systemctl restart crond ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:10","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"关闭无关服务 systemctl disable postfix \u0026\u0026 systemctl stop postfix ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:11","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"创建相关目录 mkdir -p /opt/k8s/{bin,work} /etc/{kubernetes,etcd}/cert ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:12","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"分发集群配置参数脚本 该操作在k8s-node1节点进行 集群配置参数脚本， 根据具体情况更改机器信息 脚本原地址 vim environment.sh 需要注意 NODE_IPS NODE_NAMES ETCD_ENDPOINTS ETCD_NODES IFACE CLUSTER_DNS_DOMAIN #!/usr/bin/bash # 生成 EncryptionConfig 所需的加密 key export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) # 集群各机器 IP 数组 export NODE_IPS=(192.168.7.200 192.168.7.201 192.168.7.202) # 集群各 IP 对应的主机名数组 export NODE_NAMES=(k8s-node01 k8s-node02 k8s-node03) # etcd 集群服务地址列表 export ETCD_ENDPOINTS=\"https://192.168.7.200:2379,https://192.168.7.201:2379,https://192.168.7.202:2379\" # etcd 集群间通信的 IP 和端口 export ETCD_NODES=\"k8s-node01=https://192.168.7.201:2380,k8s-node02=https://192.168.7.201:2380,k8s-node03=https://192.168.7.202:2380\" # kube-apiserver 的反向代理(kube-nginx)地址端口 export KUBE_APISERVER=\"https://127.0.0.1:8443\" # 节点间互联网络接口名称 export IFACE=\"ens192\" # etcd 数据目录 export ETCD_DATA_DIR=\"/data/k8s/etcd/data\" # etcd WAL 目录，建议是 SSD 磁盘分区，或者和 ETCD_DATA_DIR 不同的磁盘分区 export ETCD_WAL_DIR=\"/data/k8s/etcd/wal\" # k8s 各组件数据目录 export K8S_DIR=\"/data/k8s/k8s\" ## DOCKER_DIR 和 CONTAINERD_DIR 二选一 # docker 数据目录 export DOCKER_DIR=\"/data/k8s/docker\" # containerd 数据目录 export CONTAINERD_DIR=\"/data/k8s/containerd\" ## 以下参数一般不需要修改 # TLS Bootstrapping 使用的 Token，可以使用命令 head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 生成 BOOTSTRAP_TOKEN=\"41f7e4ba8b7be874fcff18bf5cf41a7c\" # 最好使用 当前未用的网段 来定义服务网段和 Pod 网段 # 服务网段，部署前路由不可达，部署后集群内路由可达(kube-proxy 保证) SERVICE_CIDR=\"10.254.0.0/16\" # Pod 网段，建议 /16 段地址，部署前路由不可达，部署后集群内路由可达(flanneld 保证) CLUSTER_CIDR=\"172.30.0.0/16\" # 服务端口范围 (NodePort Range) export NODE_PORT_RANGE=\"30000-32767\" # kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP) export CLUSTER_KUBERNETES_SVC_IP=\"10.254.0.1\" # 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配) export CLUSTER_DNS_SVC_IP=\"10.254.0.2\" # 集群 DNS 域名（末尾不带点号） export CLUSTER_DNS_DOMAIN=\"cluster.local\" # 将二进制目录 /opt/k8s/bin 加到 PATH 中 export PATH=/opt/k8s/bin:$PATH 分发到各个节点 source environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" scp environment.sh root@${node_ip}:/opt/k8s/bin/ ssh root@${node_ip} \"chmod +x /opt/k8s/bin/*\" done ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:13","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"升级内核 CentOS 7.x 内核 自带的 3.10.x 存在一些 Bugs 高版本的 docker(1.13之后) 启用了 3.10 kernel 实验室支持的 kernel memory account 功能（无法关闭），会在节点压力大（比如频繁启动关闭容器） 时候导致 cgroup memory leak 网络设备引用计数泄露，会导致类似于报错：“kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1”; 采用升级内核至 4.4.x 以上的方法 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm yum --enablerepo=elrepo-kernel install -y kernel-lt 检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置, 如果无输出，再尝试安装一次 设置开机启动内核 grub2-set-default 0 重启机器 sync reboot 查看内核 是否已经升级为 4.4.x版本 uname -a ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:4:14","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"根证书创建 需要使用 X509 证书对通信进行加密和认证； 使用 CloudFlare 的PKI工具集 cfssl 生成，CA证书只需要一份 本小节所有操作均在 k8s-node01 节点下执行 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:5:0","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"cfssl 工具集 mkdir -p /opt/k8s/cert \u0026\u0026 cd /opt/k8s/work wget https://github.com/cloudflare/cfssl/releases/download/v1.4.1/cfssl_1.4.1_linux_amd64 mv cfssl_1.4.1_linux_amd64 /opt/k8s/bin/cfssl wget https://github.com/cloudflare/cfssl/releases/download/v1.4.1/cfssljson_1.4.1_linux_amd64 mv cfssljson_1.4.1_linux_amd64 /opt/k8s/bin/cfssljson wget https://github.com/cloudflare/cfssl/releases/download/v1.4.1/cfssl-certinfo_1.4.1_linux_amd64 mv cfssl-certinfo_1.4.1_linux_amd64 /opt/k8s/bin/cfssl-certinfo chmod +x /opt/k8s/bin/* ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:5:1","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"创建配置文件 CA 配置文件用于配置根证书的使用场景（profile）和 具体参数（usage，过期时间，服务端认证，客户端认证，加密等） cd /opt/k8s/work cat \u003e ca-config.json \u003c\u003c EOF { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"876000h\" } } } } EOF 字段 含义 signing 表示该证书可以签名其他证书（ca.pem 中 CA=TRUE） server auth 表示 client 可以用该证书对 server 提供的证书进行验证 client auth 表示 server 可以用该证书对 client 提供的证书进行验证 expiry 标识证书的有效期，上述表示100年 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:5:2","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"创建证书签名请求文件 cd /opt/k8s/work cat \u003e ca-csr.json \u003c\u003cEOF { \"CN\": \"kubernetes-ca\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"opsnull\" } ], \"ca\": { \"expiry\": \"876000h\" } } EOF 字段 含义 CN Common Name kube-apiserver 从证书中提取该字段作为请求的用户名（User Name）；浏览器通过该字段验证网站是否合法 O Organization：kube-apiserver从证书中提取该字段作为请求用户所属的组 （Group） kube-apiserver 将提取的 User、Group 作为 RBAC 授权的用户标识 注意： 不同证书的 csr 文件中的 CN, C, ST, L, O, OU 组合 必须不同，否则可能出现 PEER’S CERTIFICATE HAS AN INVAILD SINGTURE 错误； 后续创建的证书，保证CN不同以达到区分 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:5:3","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"生成 CA证书以及密钥 cd /opt/k8s/work cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls ca* ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:5:4","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"分发证书 cd /opt/k8s/work source /opt/k8s/bin/environment.sh for node_ip in ${NODE_IPS[@]} do echo \"\u003e\u003e\u003e ${node_ip}\" ssh root@${node_ip} \"mkdir -p /etc/kubernetes/cert\" scp ca*.pem ca-config.json root@${node_ip}:/etc/kubernetes/cert done ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:5:5","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["Kubernetes"],"content":"参考 Kubernetes证书类型 ","date":"2019-12-09","objectID":"/k8s-install-hardway-1/:5:6","tags":["Kubernetes"],"title":"Kubernetes 二进制部署 Part1","uri":"/k8s-install-hardway-1/"},{"categories":["网络"],"content":"SoftEther VPN 简介 SoftEther VPN介绍移步此处 ","date":"2019-08-09","objectID":"/softether-vpn-split-tunnel/:0:1","tags":["VPN","网络"],"title":"Softether-VPN 拆分隧道","uri":"/softether-vpn-split-tunnel/"},{"categories":["网络"],"content":"需求来源 远程网关与本地网关 对于VPN来说，存在远程网关与本地网关的概念，以下图以SoftEther VPN 的 SecureNAT 配置为例，接入VPN后本地路由表的对比 。 如果使用远程网关，默认路由均走VPN隧道，这样VPN服务器压力较大，而且日常的网络访问都需要从VPN服务器作为出口，很显然作为远程接入公司网络该场景使用不太合理 如果使用本地网关，默认路由走的是本地的网络出口 本地网关配合静态路由 ​ 如果单纯的使用本地网关，是无法直接访问到异地的内网地址的，缺少了一步静态路由。 ​ 比如使用本地网关的情况下，公司内网存在一个地址为 192.168.7.5 , 连接VPN后，tracert一下，如图所示，经过几跳以后，在公网直接超时了。 ​ 此时我们只需要把VPN分配的虚拟网络的网关，（图中192.168.200.1 就是通过虚拟局域网前往异地内网的网关），作为本地的一条静态路由，指向如果走7网段直接通过网关192.168.200.1，添加后再次tracert 一下, 可以看到直接通过远程网关访问到了异地内网的机器 route add 192.168.7.0 mask 255.255.255.0 192.168.200.1 ​ 所以如果使用本地网关，我们需要进行一次静态路由的添加，这里存在的问题也显而易见 不添加为本机永久路由，需要每次机器重启后手动添加路由 添加为本机永久路由，可能会在某些网络环境下造成地址冲突等情况 ","date":"2019-08-09","objectID":"/softether-vpn-split-tunnel/:0:2","tags":["VPN","网络"],"title":"Softether-VPN 拆分隧道","uri":"/softether-vpn-split-tunnel/"},{"categories":["网络"],"content":"Split Tunneling ​ Split Tunneling （拆分隧道），是SoftEther-VPN中比较强悍的一个功能。具体位置在SecureNAT配置界面就可以找到。 ​ 简单来讲 拆分隧道可以理解为推送静态路由，接入 VPN 以后，server端会推送设置的静态路由到client端，断开VPN后，推送的静态路由失效，完美的解决了上述问题带来的痛点。 ​ 但是对于Softether VPN 来说，拆分隧道功能并不适合开源版本，从网上查到的信息，天朝跟岛国不可以使用该功能在内的一部分功能(当然仅限于官方下载的编译好的版本，对于自己进行源码编译是不限制的) ","date":"2019-08-09","objectID":"/softether-vpn-split-tunnel/:0:3","tags":["VPN","网络"],"title":"Softether-VPN 拆分隧道","uri":"/softether-vpn-split-tunnel/"},{"categories":["网络"],"content":"解除限制 下载源码 下载地址， 组件选择 Source Code of SoftEther VPN 如果是生产环境在用，建议下载在用版本的源码 删除限制部分代码 解压后，在以下路径中找到Server.c文件，编辑Server部分代码 /your_tar_path/src/Cedar/Server.c 可以看出Server端代码在以下 两个函数中 出现了关键词 CN 与 JP void SiGetCurrentRegion(CEDAR *c, char *region, UINT region_size) bool SiIsEnterpriseFunctionsRestrictedOnOpenSource(CEDAR *c) SiIsEnterpriseFunctionsRestrictedOnOpenSource函数中调用了SiGetCurrentRegion函数，最终的逻辑判断还是发生在SiIsEnterpriseFunctionsRestrictedOnOpenSource函数该段代码 if (StrCmpi(region, \"JP\") == 0 || StrCmpi(region, \"CN\") == 0) { ret = true; } 我们直接把 ret 的赋值改为 false；当然更改方法多种多样 编译 CentOS yum -y groupinstall \"Development Tools\" yum -y install readline-devel ncurses-devel openssl-devel ./configure make Debian apt-get install gcc automake autoconf libtool make libreadline-dev libssl-dev zlib1g-dev lib32ncurses5-dev -y ./configure make 部署 编译完成后，会在如下路径生成vpnserver 以及hamcore.se2文件 bin/vpnserver/ 直接用上述两个文件替换掉原部署的vpnserver以及hamcore.se2即可 注意: 替换前注意备份原目录，替换前注意停止vpnserver服务 测试 再次Manager连接进行路由推送，并没有弹出窗口限制 客户端拨入VPN，再次查看路由表，发现路由已经推送过来了；断开VPN后该条路由被清理 ","date":"2019-08-09","objectID":"/softether-vpn-split-tunnel/:0:4","tags":["VPN","网络"],"title":"Softether-VPN 拆分隧道","uri":"/softether-vpn-split-tunnel/"},{"categories":["网络"],"content":"Docker镜像 ​ 可以直接拉取镜像以便在 Docker 以及 K8s 集群中使用 docker pull abyssviper/softethervpn ​ 详细使用说明请参考：https://hub.docker.com/r/abyssviper/softethervpn ","date":"2019-08-09","objectID":"/softether-vpn-split-tunnel/:0:5","tags":["VPN","网络"],"title":"Softether-VPN 拆分隧道","uri":"/softether-vpn-split-tunnel/"}]